{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [Firstname Lastname](https://) for the 2022 Text Analysis Pedagogy Institute, with support from the [National Endowment for the Humanities](https://neh.gov), [JSTOR Labs](https://labs.jstor.org/), and [University of Arizona Libraries](https://new.library.arizona.edu/).\n",
    "\n",
    "For questions/comments/improvements, email author@email.address.<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# `NLP with spaCy` `2`\n",
    "\n",
    "This is lesson `2` of 3 in the educational series on `Natural Language Processing (NLP)`. This notebook is intended `to teach the basics of NLP and the spaCy library.`. \n",
    "\n",
    "**Audience:** `Teachers` / `Learners` / `Researchers`\n",
    "\n",
    "**Use case:** `Tutorial`\n",
    "\n",
    "`Include the use case definition from [here](https://constellate.org/docs/documentation-categories)`\n",
    "\n",
    "**Difficulty:** `Beginner`\n",
    "\n",
    "`Beginner assumes users are relatively new to Python and Jupyter Notebooks. The user is helped step-by-step with lots of explanatory text.`\n",
    "\n",
    "`Intermediate assumes users are familiar with Python and have been programming for 6+ months. Code makes up a larger part of the notebook and basic concepts related to Python are not explained.`\n",
    "\n",
    "`Advanced assumes users are very familiar with Python and have been programming for years, but they may not be familiar with the process being explained.`\n",
    "\n",
    "**Completion time:** `90 minutes`\n",
    "\n",
    "**Knowledge Required:** \n",
    "```\n",
    "* Python basics (variables, flow control, functions, lists, dictionaries)\n",
    "```\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "```\n",
    "* Basic file operations (open, close, read, write)\n",
    "```\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "```\n",
    "1. Understand spaCy Pipelines\n",
    "2. Understand spaCy Components\n",
    "3. Understand NER generally\n",
    "```\n",
    "**Research Pipeline:**\n",
    "```\n",
    "N/A\n",
    "```\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Required Python Libraries\n",
    "`List out any libraries used and what they are used for`\n",
    "* [spaCy](https://spacy.io/) for performing [Natural Language Processing (NLP)](https://docs.constellate.org/key-terms/#nlp).\n",
    "\n",
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a220f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\wma22\\anaconda3\\lib\\site-packages (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (2.4.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (62.1.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (8.0.17)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wma22\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\wma22\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "### Install Libraries ###\n",
    "\n",
    "# Using !pip installs\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "# Using %%bash magic with apt-get and yes prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5480e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries ###\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53edaa2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "```\n",
    "Introduce the lesson topic. Answer questions such as:\n",
    "* Why is it useful? \n",
    "* Why should we learn it? \n",
    "* Who might use it? \n",
    "* Where has it been used by scholars/industry?\n",
    "* What do we need to do it?\n",
    "* What subjects are included in the notebooks?\n",
    "* What is not in this notebook? Where should we look for it?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c98f19",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part One: spaCy Pipelines\n",
    "\n",
    "In Part One of this notebook, we will be learning about the various pipelines in spaCy. As we have seen, spaCy offers both heuristic (rules-based) and machine learning natural language processing solutions. These solutions are activated by pipes. In this notebook, you will learn about pipes and pipelines generally and the ones offered by spaCy specifically. In a later notebook, we will explore how you can create custom pipes and add them to a spaCy pipeline. Before we jump in, let's import spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece38014-4856-4962-96c5-d494c67b3cbd",
   "metadata": {},
   "source": [
    "## Standard Pipes (Components and Factories) Available from spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647883fb-0f00-4e05-ab0b-71a706ffab57",
   "metadata": {},
   "source": [
    "SpaCy is much more than an NLP framework. It is also a way of designing and implementing complex pipelines. A **pipeline** is a sequence of **pipes**, or actors on data, that make alterations to the data or extract information from it. In some cases, later pipes require the output from earlier pipes. In other cases, a pipe can exist entirely on its own. An example can be see in the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ebbe93-ca67-46b4-9daa-008c8b64f2d6",
   "metadata": {},
   "source": [
    "```{image} ./images/sample_pipeline.png\n",
    ":alt: pipeline\n",
    ":class: bg-primary\n",
    ":width: 600px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a7c41-5a0b-46f9-b931-9dec2e02b6a5",
   "metadata": {},
   "source": [
    "Here, we see an input, in this case a sentence, enter the pipeline from the left. Two pipes are activated on this, a rules-based named entity recognizer known as an EntityRuler which finds entities and an EntityLinker pipe that identifies what entity that is to perform toponym resolution. The sentence is then outputted with the sentence and the entities annotated. At this point, we could use the doc.ents feature to find the entities in our sentence. In spaCy, you will often use pipelines that are more sophisticated than this. You will specifically use a Tok2Vec input layer to vectorize your input sentence. This will allow machine learning pipes to make predictions.\n",
    "\n",
    "Below is a complete list of the AttributeRuler pipes available to you from spaCy and the Matchers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddddb36-d863-4688-b09a-52479e7e1b0d",
   "metadata": {},
   "source": [
    "### Attribute Rulers\n",
    "* Dependency Parser\n",
    "* EntityLinker\n",
    "* EntityRecognizer\n",
    "* EntityRuler\n",
    "* Lemmatizer\n",
    "* Morpholog\n",
    "* SentenceRecognizer\n",
    "* Sentencizer\n",
    "* SpanCategorizer\n",
    "* Tagger\n",
    "* TextCategorizer\n",
    "* Tok2Vec\n",
    "* Tokenizer\n",
    "* TrainablePipe\n",
    "* Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f68c1-fe79-4997-8137-b97ab6d25fa2",
   "metadata": {},
   "source": [
    "### Matchers\n",
    "* DependencyMatcher\n",
    "* Matcher\n",
    "* PhraseMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a974bc-8a85-4594-bbd0-1c219ee9a7f3",
   "metadata": {},
   "source": [
    "## How to Add Pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0840ea1c-5528-4c62-8fab-7f80ab528149",
   "metadata": {},
   "source": [
    "In most cases, you will use an off-the-shelf spaCy model. In some cases, however, an off-the-shelf model will not fill your needs or will perform a specific task very slowly. A good example of this is sentence tokenization. Imagine if you had a document that was around 1 million sentences long. Even if you used the small English model, your model would take a long time to process those 1 million sentences and separate them. In this instance, you would want to make a blank English model and simply add the Sentencizer to it. The reason is because each pipe in a pipeline will be activated (unless specified) and that means that each pipe from Dependency Parser to named entity recognition will be performed on your data. This is a serious waste of computational resources and time. The small model may take hours to achieve this task. By creating a blank model and simply adding a Sentencizer to it, you can reduce this time to merely minutes.\n",
    "\n",
    "To demonstrate this process, let's first create a blank model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e83a55f2-f98e-4dec-b185-ebcec1f33bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c5589-0f08-4db9-80d2-2a7605106f33",
   "metadata": {},
   "source": [
    "Here, notice that we have used spacy.blank, rather than spacy.load. When we create a blank model, we simply pass the two letter combination for a language, in this case, en for English. Now, let's use the add_pipe() command to add a new pipe to it. We will simply add a sentencizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0401d8da-81a7-4b71-a361-b1ef026d653c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x2167b5468c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53ae6b0b-1779-4666-be53-7502b0e0e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "s = requests.get(\"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\")\n",
    "soup = BeautifulSoup(s.content).text.replace(\"-\\n\", \"\").replace(\"\\n\", \" \")\n",
    "nlp.max_length = 5278439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e75544c8-d293-4d27-a2c3-72b5225c8a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94133\n",
      "Wall time: 7.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc = nlp(soup)\n",
    "print (len(list(doc.sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b03a958-6018-45a0-ab08-6dd52cd398b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load(\"en_core_web_sm\")\n",
    "nlp2.max_length = 5278439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d239a4f-7e3a-45b2-b3ab-d2158626e4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112074\n",
      "Wall time: 47min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc = nlp2(soup)\n",
    "print (len(list(doc.sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0c6b37-dfcd-48e4-87c2-752966283320",
   "metadata": {},
   "source": [
    "The difference in time here is remarkable. Our text string was around 5.2 million characters. The blank model with just the Sentencizer completed its task in 7.54 seconds and found around 94k sentences. The small English model, the most efficient one offered by spaCy, did the same task in 46 minutes and 15 seconds and found around 112k sentences. The small English model, in other words, took approximately 380 times longer. \n",
    "\n",
    "Often times you need to find sentences quickly, not necessarily accurately. In these instances, it makes sense to know tricks like the one above. This notebook concludes part one of this book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c35bbc8-3cfb-4b5d-a6aa-9a1881a8523e",
   "metadata": {},
   "source": [
    "## Examining a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e51f082-277f-466d-92b5-d87732afd58e",
   "metadata": {},
   "source": [
    "In spaCy, we have a few different ways to study a pipeline. If we want to do this in a script, we can do the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4478e45-433b-415b-9f30-5f9458f6e510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2.analyze_pipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce92c9a-8438-4388-b6cf-f607f830141e",
   "metadata": {},
   "source": [
    "Note the dictionary structure. This tells us not only what is inside the pipeline, but its order. Each key after \"summary\" is a pipe. The value is a dictionary. This dictionary tells us a few different things. All of these value dictionaries state: \"assigns\" which corresponds to a value of what that particular pipe assigns to the token and doc as it passes through the pipeline. In some cases, there will be a key of \"scores\" in the dictionary. This indicates how the machine learning model was evaluated. We will learn more about model evaluation in our machine learning section below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a40d04-d05e-4459-a493-78e118b100ec",
   "metadata": {},
   "source": [
    "# Part Two: The spaCy EntityRuler\n",
    "\n",
    "The Python library spaCy offers a few different methods for performing rules-based NER. One such method is via its EntityRuler.\n",
    "\n",
    "The **EntityRuler** is a spaCy factory that allows one to create a set of patterns with corresponding labels. A **factory** in spaCy is a set of classes and functions preloaded in spaCy that perform set tasks. In the case of the EntityRuler, the factory at hand allows the user to create an EntityRuler, give it a set of instructions, and then use this instructions to find and label entities.\n",
    "\n",
    "Once the user has created the EntityRuler and given it a set of instructions, the user can then add it to the spaCy pipeline as a new pipe. I have spoken in the past notebooks briefly about pipes, but perhaps it is good to address them in more detail here.\n",
    "\n",
    "A **pipe** is a component of a **pipeline**. A pipeline's purpose is to take input data, perform some sort of operations on that input data, and then output those operations either as a new data or extracted metadata. A pipe is an individual component of a pipeline. In the case of spaCy, there are a few different pipes that perform different tasks. The tokenizer, tokenizes the text into individual tokens; the parser, parses the text, and the NER identifies entities and labels them accordingly. All of this data is stored in the Doc object as we saw in Notebook 01_01 of this series.\n",
    "\n",
    "It is important to remember that pipelines are sequential. This means that components earlier in a pipeline affect what later components receive. Sometimes this sequence is essential, meaning later pipes depend on earlier pipes. At other times, this sequence is not essential, meaning later pipes can function without earlier pipes. It is important to keep this in mind as you create custom spaCy models (or any pipeline for that matter).\n",
    "\n",
    "In this notebook, we will be looking closely at the EntityRuler as a component of a spaCy model's pipeline. Off-the-shelf spaCy models come preloaded with an NER model; they do not, however, come with an EntityRuler. In order to incorperate an EntityRuler into a spaCy model, it must be created as a new pipe, given instructions, and then added to the model. Once this is complete, the user can save that new model with the EntityRuler to the disk.\n",
    "\n",
    "The full documentation of spaCy EntityRuler can be found here: https://spacy.io/api/entityruler .\n",
    "\n",
    "This notebook with synthesize this documentation for non-specialists and provide some examples of it in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd1b6e2-a15d-4d4b-9d03-7ff737f07415",
   "metadata": {},
   "source": [
    "## Demonstration of EntityRuler in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e2920-f049-41ac-9fde-332f9ccb272c",
   "metadata": {},
   "source": [
    "In the code below, we will introduce a new pipe into spaCy's off-the-shelf small English model. The purpose of this EntityRuler will be to identify small villages in Poland correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb027cf0-f40b-4a90-aaaf-25436d830355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poland GPE\n"
     ]
    }
   ],
   "source": [
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Sample text\n",
    "text = \"The village of Treblinka is in Poland. Treblinka was also an extermination camp.\"\n",
    "\n",
    "#Create the Doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228cbc00-ce61-4b56-ab76-3ef3431bca19",
   "metadata": {},
   "source": [
    "*Depending on the version of model you are using, some results may vary.*\n",
    "\n",
    "The output from the code above demonstrates spaCy's small model's to identify Treblinka, which is a small village in Poland. As the sample text indicates, it was also an extermination camp during WWII. In the first sentence, the spaCy model tagged Treblinka as an LOC (location) and in the second it was missed entirely. Both are either imprecise or wrong. I would have accepted ORG for the second sentence, as spaCy's model does not know how to classify an extermination camp, but what these results demonstrate is the model's failure to generalize on data. The reason? There are a few, but I suspect the model never encountered the word Treblinka.\n",
    "\n",
    "This is a common problem in NLP for specific domains. Often times the domains in which we wish to deploy models, off-the-shelf models will fail because they have not been trained on domain-specific texts. We can resolve this, however, either via spaCy's EntityRuler or via training a new model. As we will see over the next few notebooks, we can use spaCy's EntityRuler to easily achieve both.\n",
    "\n",
    "For now, let's first remedy the issue by giving the model instructions for correctly identifying Treblinka. For simplicity, we will use spaCy's GPE label. In a later notebook, we will teach a model to correctly identify Treblinka in the latter context as a concentration camp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2a8e727-3cf1-4e94-9633-269faec8047e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treblinka GPE\n",
      "Poland GPE\n",
      "Treblinka GPE\n"
     ]
    }
   ],
   "source": [
    "#Import the requisite library\n",
    "import spacy\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Sample text\n",
    "text = \"The village of Treblinka is in Poland. Treblinka was also an extermination camp.\"\n",
    "\n",
    "#Create the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"Treblinka\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabba5ed-88c9-4437-8580-19259e0817a5",
   "metadata": {},
   "source": [
    "If you executed the code above and found that you had the same output, then you did everything correctly. This method has failed. Why? The answer comes back to the concept of pipelines. We created and added the EntityRuler to the spaCy model's pipeline, but by default, spaCy add's a new pipe to the end of the pipeline. In order to visualize the pipeline, let's use spaCy's analyze_pipes()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a06f09ad-60ff-4ce9-9fb4-18af433edc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False},\n",
       "  'entity_ruler': {'assigns': ['doc.ents', 'token.ent_type', 'token.ent_iob'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': [],\n",
       "  'entity_ruler': []},\n",
       " 'attrs': {'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner', 'entity_ruler'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner', 'entity_ruler'], 'requires': []}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f47dcc-3832-41e8-bf48-3de8e3b89f14",
   "metadata": {},
   "source": [
    "This can be a bit difficult to read at first, but what it shows us is the order in which our pipes are set up and a few other key pieces of information about each pipe. If we locate \"ner\", we notice that \"entity_ruler\" sits behind it.\n",
    "\n",
    "In order for our EntityRuler to have primacy, we have to assign it to after the \"ner\" pipe, as the example below shows in this line:\n",
    "\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", **after=\"ner\"**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5779a00-5205-4c6f-8f26-27b3d849319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treblinka GPE\n",
      "Poland GPE\n",
      "Treblinka GPE\n"
     ]
    }
   ],
   "source": [
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Sample text\n",
    "text = \"The village of Treblinka is in Poland. Treblinka was also an extermination camp.\"\n",
    "\n",
    "#Create the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", after=\"ner\")\n",
    "\n",
    "#List of Entities and Patterns\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"Treblinka\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e19f82-2e22-490a-967b-fe6d3693cee6",
   "metadata": {},
   "source": [
    "Notice now that our EntityRuler is functioning before the \"ner\" pipe and is, therefore, prefinding entities and labeling them before the NER gets to them. Because it comes earlier in the pipeline, its metadata holds primacy over the later \"ner\" pipe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f183dc13-8b24-4569-b17a-0bf57fcd9040",
   "metadata": {},
   "source": [
    "## Introducing Complex Rules and Variance to the EntityRuler (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a969e6ff-43cb-4360-a737-bc291b075d8c",
   "metadata": {},
   "source": [
    "In some instances, labels may have a set type of variance that follow a distinct pattern or sets of patterns. One such example (included in the spaCy documentation) is phone numbers. In the United States, phone numbers have a few forms. The standard formal method is (xxx)-xxx-xxxx, but it is not uncommon to see xxx-xxx-xxxx or xxxxxxxxxx. If the owner of the phone number is giving that same number to someone outside the US, then +1(xxx)-xxx-xxxx.\n",
    "\n",
    "If you are working within a United States domain, you can pass RegEx formulas to the pattern matcher to grab all of these instances.\n",
    "\n",
    "The spaCy EntityRuler also allows the user to introduce a variety of complex rules and variances (via, among other things, RegEx) by passing the rules to the pattern. There are many arguments that one can pass to the patterns. For a complete list, see: https://spacy.io/usage/rule-based-matching . To expiremnet with how these work, I recommend using the spaCy Matcher demo: https://explosion.ai/demos/matcher .\n",
    "\n",
    "In the example below we work with one example from the spaCy documentation in which we extract a phone number from a text. This same task can be done via RegEx as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5a2d71b-9019-4e2a-8d49-2198597ab5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(555) 555-5555 PHONE_NUMBER\n"
     ]
    }
   ],
   "source": [
    "#Sample text\n",
    "text = \"This is a sample number (555) 555-5555.\"\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "#Create the Ruler and Add it\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
    "patterns = [\n",
    "                {\"label\": \"PHONE_NUMBER\", \"pattern\": [{\"ORTH\": \"(\"}, {\"SHAPE\": \"ddd\"}, {\"ORTH\": \")\"}, {\"SHAPE\": \"ddd\"},\n",
    "                {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\"}]}\n",
    "            ]\n",
    "#add patterns to ruler\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "\n",
    "\n",
    "#create the doc\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca42822",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "`I know we covered a lot in this notebook and the best way to understand its contents in depth is to apply it to your own domain, or area of expertise. I encourage you to select a text (or texts) that you use in your own research and try to apply the methods covered in this notebook to those particular texts. I would highly encourage you to do this before moving on to the next notebook.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a6d92",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
